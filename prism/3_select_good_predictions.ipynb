{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import cv2\n",
    "import sys\n",
    "from itertools import groupby, repeat\n",
    "from operator import itemgetter\n",
    "sys.path.append('./src')\n",
    "import procrustes as proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/data/LiftFly3D/prism/'\n",
    "data_dir = '/mnt/NAS/SG/prism_data/'\n",
    "scorer_bottom = 'DLC_resnet50_jointTrackingDec13shuffle1_200000'\n",
    "scorer_side = 'DLC_resnet50_sideJointTrackingDec17shuffle1_200000'\n",
    "\n",
    "#joints whose confidence to consider\n",
    "leg_tips = ['tarsus tip front L', 'tarsus tip mid L', 'tarsus tip back L',\n",
    "          'tarsus tip front R', 'tarsus tip mid R', 'tarsus tip back R']\n",
    "\n",
    "crop_positions = ['/bottom_view/videos/crop_loc_191125_PR_Fly1_001_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_002_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_003_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_004_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_001_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_002_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_003_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_004_prism.txt']\n",
    "\n",
    "videos_side = ['side_view/videos/video_191125_PR_Fly1_001_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_002_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_003_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_004_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_001_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_002_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_003_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_004_prism']\n",
    "\n",
    "videos_bottom =  ['bottom_view/videos/video_191125_PR_Fly1_001_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_002_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_003_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_004_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_001_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_002_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_003_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_004_prism']\n",
    "\n",
    "images_side = ['191125_PR/Fly1/001_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/002_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/003_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/004_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly2/001_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/002_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/003_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/004_prism/behData/images/side_view_prism_data_191125_PR_Fly2/']\n",
    "\n",
    "images_bottom =  ['191125_PR/Fly1/001_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/002_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/003_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/004_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly2/001_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/002_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/003_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/004_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/']\n",
    "\n",
    "assert len(videos_side)==len(videos_bottom), 'Number of video files must be the same from side and bottom!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop_pos(file):\n",
    "    f=open(file, \"r\")\n",
    "    contents =f.readlines()\n",
    "    im_file = []\n",
    "    x_pos = []\n",
    "    for i in range(4,len(contents)):\n",
    "        line = contents[i][:-1].split(' ')\n",
    "        im_file.append(line[0])\n",
    "        x_pos.append(line[1])\n",
    "        \n",
    "    return im_file, x_pos\n",
    "\n",
    "\n",
    "def flip_LR(data):\n",
    "    cols = list(data.columns)\n",
    "    half = int(len(cols)/2)\n",
    "    tmp = data.loc[:,cols[:half]].values\n",
    "    data.loc[:,cols[:half]] = data.loc[:,cols[half:]].values\n",
    "    data.loc[:,cols[half:]] = tmp\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def rotate_to_horizontal(pts2d, exp):\n",
    "    \n",
    "    #access corresponding image file\n",
    "    idx = pts2d.name \n",
    "    im_file, _ = read_crop_pos(home_dir + crop_positions[exp])\n",
    "    im_crop_bottom = cv2.imread(data_dir + images_bottom[exp] + im_file[idx],0)\n",
    "    \n",
    "    pts2d = proc.center_and_align(pts2d, im_crop_bottom)\n",
    "    \n",
    "    return pts2d\n",
    "\n",
    "\n",
    "def orient_left(bottom, side):\n",
    "    #rotate flies pointing right\n",
    "    side_R_lk = side.loc[:,(leg_tips[3:],'likelihood')] #high confidence on R joints means fly points right\n",
    "    flip_idx = (side_R_lk>th1).sum(1)==3\n",
    "        \n",
    "    theta = np.radians(180)\n",
    "    cos, sin = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((cos, -sin), (sin, cos)))     \n",
    "    \n",
    "    if np.sum(flip_idx) != 0:\n",
    "        tmp = bottom.loc[flip_idx,(slice(None),['x','y'])].to_numpy()\n",
    "        tmp = np.reshape(tmp, [-1, 2])\n",
    "        mu = tmp.mean(axis=0)\n",
    "        tmp = np.matmul(tmp-mu,R)# + mu\n",
    "        tmp = np.reshape( tmp, [-1, 60] )\n",
    "        bottom.loc[flip_idx,(slice(None),['x','y'])] = tmp\n",
    "\n",
    "    return bottom, side, flip_idx\n",
    " \n",
    "\n",
    "def procrustes_on_epochs(data, epochs):\n",
    "    xy = data.loc[:,(slice(None),['x','y'])]\n",
    "    for e in epochs:\n",
    "        for step in range(1,len(e)):\n",
    "            X = xy.loc[e[step]-1,:]\n",
    "            Xtransf = xy.loc[e[step],:]\n",
    "                \n",
    "            X = X.to_numpy().reshape([-1, 2])\n",
    "            Xtransf = Xtransf.to_numpy().reshape([-1, 2])\n",
    "                \n",
    "            _, _, T, _, c = proc.compute_similarity_transform(Xtransf, X)\n",
    "            Xtransf = (T@Xtransf.T).T\n",
    "\n",
    "            xy.loc[e[step],:] = Xtransf.reshape(-1, 60).flatten()\n",
    "                \n",
    "    data.loc[:,(slice(None),['x','y'])] = bottom_xy\n",
    "    return data\n",
    "    \n",
    "        \n",
    "def get_epochs(data):\n",
    "    data_idx = list(data.index)\n",
    "    epochs = []\n",
    "    for k, g in groupby(enumerate(data_idx), lambda ix : ix[0] - ix[1]):\n",
    "        epochs.append(list(map(itemgetter(1), g)))\n",
    "        \n",
    "    return epochs\n",
    "\n",
    "\n",
    "def select_best_data(bottom, side, th1, th2):\n",
    "    \n",
    "    #split L and R (remove if we include flipping)\n",
    "    side_L_lk = side.loc[:,(leg_tips[:3],'likelihood')]\n",
    "    side_R_lk = side.loc[:,(leg_tips[3:],'likelihood')]\n",
    "    bottom_lk = bottom.loc[:,(leg_tips,'likelihood')]\n",
    "    \n",
    "    #select for high likelihood frames\n",
    "    #mask = ((side_L_lk>th1).sum(1)==3) & ((bottom_lk>th1).sum(1)==6) #only flies pointing left\n",
    "    #mask = ((side_R_lk>th1).sum(1)==3) & ((bottom_lk>th1).sum(1)==6) #only flies pointing right\n",
    "    mask = ( (((side_L_lk>th1).sum(1)==3) & ((side_R_lk>th1).sum(1)==0)) | \\\n",
    "             (((side_R_lk>th1).sum(1)==3) & ((side_L_lk>th1).sum(1)==0)) ) & \\\n",
    "             ((bottom_lk>th1).sum(1)==6)\n",
    "    side = side[mask].dropna()\n",
    "    bottom = bottom[mask].dropna()\n",
    "    \n",
    "    #sometimes DLC mixes up limbs so take only those frames there the x coordinate matches on side and bottom views\n",
    "    diff_L = np.abs(bottom.loc[:,(leg_tips[:3],'x')].values - side.loc[:,(leg_tips[3:],'x')].values)\n",
    "    diff_R = np.abs(bottom.loc[:,(leg_tips[3:],'x')].values - side.loc[:,(leg_tips[:3],'x')].values)\n",
    "    mask = ((diff_L<th2).sum(1)==3) | ((diff_R<th2).sum(1)==3)\n",
    "    side = side[mask].dropna()\n",
    "    bottom = bottom[mask].dropna()\n",
    "    \n",
    "    assert side.shape[0]==bottom.shape[0], 'Number of rows must match in filtered data!'\n",
    "    \n",
    "    return bottom, side\n",
    "    \n",
    "\n",
    "def augment(bottom, typ, rng):\n",
    "    tmp = bottom.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "    bottom_old = bottom.copy()\n",
    "    \n",
    "    if typ == 'rot':\n",
    "        _rng = rng\n",
    "    if typ == 'noise':\n",
    "        _rng = range(len(rng))\n",
    "    \n",
    "    for angle in _rng: \n",
    "        bottom_rot = bottom_old.copy()\n",
    "        \n",
    "        if typ=='noise':\n",
    "            bottom_rot.loc[:,(slice(None),['x','y'])] = tmp + np.random.normal(0,6,size=tmp.shape)\n",
    "        \n",
    "        if typ=='rot':\n",
    "            theta = np.radians(angle)\n",
    "            cos, sin = np.cos(theta), np.sin(theta)\n",
    "            R1 = np.array(((cos, -sin), (sin, cos)))\n",
    "            tmp1 = np.reshape(tmp, [-1, 2])\n",
    "            mu = tmp1.mean(axis=0)\n",
    "            tmp1 = np.matmul(tmp1-mu,R1)# + mu\n",
    "            tmp1 = np.reshape( tmp1, [-1, 60] )\n",
    "            bottom_rot.loc[:,(slice(None),['x','y'])] = tmp1\n",
    "            \n",
    "        bottom = bottom.append(bottom_rot) #append\n",
    "    \n",
    "    return bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode= 'train' #0: train, 1: prediction, 2: DLC_video, 3: train_low_res\n",
    "\n",
    "if mode=='train':\n",
    "    th1 = 0.95 #confidence threshold\n",
    "    th2 = 10 #max L-R discrepancy in x coordinate\n",
    "    align=1\n",
    "    fine_align=0\n",
    "    nice_epochs=0\n",
    "    aug_angles=1\n",
    "    aug_noise=0\n",
    "if mode=='prediction':\n",
    "    th1 = 0.5 #confidence threshold\n",
    "    th2 = 10 #max L-R discrepancy in x coordinate\n",
    "    align=1\n",
    "    fine_align=1\n",
    "    nice_epochs=1\n",
    "    aug_angles=0\n",
    "    aug_noide=0\n",
    "if mode=='DLC_video':\n",
    "    th1 = 0.5 #confidence threshold\n",
    "    th2 = 10 #max L-R discrepancy in x coordinate\n",
    "    align=0\n",
    "    fine_align=0\n",
    "    nice_epochs=1\n",
    "    aug_angles=0\n",
    "    aug_noide=0\n",
    "if mode=='train_low_res':\n",
    "    th1 = 0.95 #confidence threshold\n",
    "    th2 = 10 #max L-R discrepancy in x coordinate\n",
    "    align=1\n",
    "    fine_align=0\n",
    "    nice_epochs=0\n",
    "    aug_angles=1\n",
    "    aug_noise=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:32<03:47, 32.46s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [02:46<06:18, 63.02s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "side = pd.DataFrame()\n",
    "bottom = pd.DataFrame()\n",
    "flip_idx = None\n",
    "for i in tqdm(range(len(videos_side))):\n",
    "    #load data of side and bottom view\n",
    "    _side = pd.read_hdf(home_dir + videos_side[i] + scorer_side + '.h5')\n",
    "    _bottom = pd.read_hdf(home_dir + videos_bottom[i] + scorer_bottom + '.h5')\n",
    "    _side = _side.droplevel('scorer',axis=1) \n",
    "    _bottom = _bottom.droplevel('scorer',axis=1) \n",
    "    \n",
    "    #select for high confidence datapoints\n",
    "    _bottom, _side = select_best_data(_bottom, _side, th1, th2)\n",
    "    \n",
    "    #flip left and right side of bottom due to prism reflection\n",
    "    _bottom = flip_LR(_bottom)\n",
    "    \n",
    "    #get epochs (list of lists of consecutive timesteps)\n",
    "    epochs = get_epochs(_bottom)\n",
    "      \n",
    "    #align horizontally\n",
    "    if align: #1 for training and prediction, 0 for making of DLC video\n",
    "        _bottom.loc[:,(slice(None),['x','y'])] = _bottom.loc[:,(slice(None),['x','y'])].apply(lambda x: rotate_to_horizontal(x, i), axis=1)\n",
    "          \n",
    "        #orient all flies left\n",
    "        _bottom, _side, flip_idx = orient_left(_bottom, _side)\n",
    "        \n",
    "    #procrustes to fine-align flies \n",
    "    if fine_align: #0 for training, 1 for prediction\n",
    "        _bottom = procrustes_on_epochs(_bottom, epochs)\n",
    "            \n",
    "    #take only those frames with at least 10 consecutive frames\n",
    "    if nice_epochs: #0 for training, 1 for prediction\n",
    "        long_epochs = []\n",
    "        for e in range(len(epochs)-1):\n",
    "            if (len(epochs[e]) >= 5) | ((len(epochs[e])<5) & ((epochs[e+1][0] - epochs[e][-1]<5))):\n",
    "                long_epochs += epochs[e]\n",
    "                long_epochs += epochs[e+1]\n",
    "            \n",
    "        long_epochs=np.unique(long_epochs)  \n",
    "    \n",
    "        _bottom=_bottom.loc[long_epochs,:]\n",
    "        _side=_side.loc[long_epochs,:]\n",
    "        print('long epochs')\n",
    "    \n",
    "    #save indices for later\n",
    "    index.append(_bottom.index.values)\n",
    "\n",
    "    #augment with rotated flies +/- 10 deg\n",
    "    if aug_angles: #1 for training, 0 for prediction\n",
    "        angles = [-10, 10]\n",
    "        _bottom = augment(_bottom, typ='rot', rng=angles)\n",
    "        _side = pd.concat([_side]*(len(angles)+1))\n",
    "        flip_idx = pd.concat([flip_idx]*(len(angles)+1))\n",
    "       \n",
    "    #augment with noise to scale down\n",
    "    if aug_noise: #1 only if applied to lower resolution setups, 0 otherwise\n",
    "        samples = 6\n",
    "        _bottom = augment(bottom, typ='noise', rng=samples)\n",
    "        _side = pd.concat([_side]*(samples+1))\n",
    "        flip_idx = pd.concat([flip_idx]*(samples+1))\n",
    "    \n",
    "    #convert & save to DF3D format\n",
    "    side_np = _side.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "    z = _side.loc[:,(slice(None),'y')].to_numpy()\n",
    "    side_np = np.stack((side_np[:,::2], side_np[:,1::2]), axis=2)\n",
    "\n",
    "    bottom_np = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "    bottom_np = np.stack((bottom_np[:,::2], bottom_np[:,1::2]), axis=2)\n",
    "    \n",
    "    poses = {'points2d': np.stack((bottom_np, side_np), axis=0),\n",
    "             'points3d': np.concatenate((bottom_np, z[:,:,None]), axis=2),\n",
    "             'index':_side.index.values,\n",
    "             'flip_idx': np.array(flip_idx)\n",
    "            }\n",
    "    \n",
    "    #save\n",
    "    pickle.dump(poses,open(home_dir + videos_side[i].split('/')[-1][6:] + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coord1 = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()[2,:].reshape(-1,2)\n",
    "\n",
    "x1 = coord1[:,0]\n",
    "y1 = coord1[:,1]\n",
    "\n",
    "plt.scatter(x1,y1)\n",
    "\n",
    "coord2 = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()[3,:].reshape(-1,2)\n",
    "\n",
    "\n",
    "_, _, T, _, c = proc.compute_similarity_transform(coord2, coord1 )\n",
    "coord2 = (T@coord2.T).T\n",
    "\n",
    "x2 = coord2[:,0]\n",
    "y2 = coord2[:,1]\n",
    "plt.scatter(x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
