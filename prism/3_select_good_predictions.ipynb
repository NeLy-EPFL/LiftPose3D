{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import cv2\n",
    "from math import atan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/data/LiftFly3D/prism/'\n",
    "data_dir = '/mnt/NAS/SG/prism_data/'\n",
    "scorer_bottom = 'DLC_resnet50_jointTrackingDec13shuffle1_200000'\n",
    "scorer_side = 'DLC_resnet50_sideJointTrackingDec17shuffle1_200000'\n",
    "\n",
    "#joints whose confidence to consider\n",
    "leg_tips = ['tarsus tip front L', 'tarsus tip mid L', 'tarsus tip back L',\n",
    "          'tarsus tip front R', 'tarsus tip mid R', 'tarsus tip back R']\n",
    "\n",
    "crop_positions = ['/bottom_view/videos/crop_loc_191125_PR_Fly1_001_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_002_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_003_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly1_004_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_001_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_002_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_003_prism.txt',\n",
    "                  '/bottom_view/videos/crop_loc_191125_PR_Fly2_004_prism.txt']\n",
    "\n",
    "videos_side = ['side_view/videos/video_191125_PR_Fly1_001_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_002_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_003_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly1_004_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_001_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_002_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_003_prism',\n",
    "               'side_view/videos/video_191125_PR_Fly2_004_prism']\n",
    "\n",
    "videos_bottom =  ['bottom_view/videos/video_191125_PR_Fly1_001_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_002_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_003_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly1_004_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_001_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_002_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_003_prism',\n",
    "                  'bottom_view/videos/video_191125_PR_Fly2_004_prism']\n",
    "\n",
    "images_side = ['191125_PR/Fly1/001_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/002_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/003_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly1/004_prism/behData/images/side_view_prism_data_191125_PR_Fly1/',\n",
    "               '191125_PR/Fly2/001_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/002_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/003_prism/behData/images/side_view_prism_data_191125_PR_Fly2/',\n",
    "               '191125_PR/Fly2/004_prism/behData/images/side_view_prism_data_191125_PR_Fly2/']\n",
    "\n",
    "images_bottom =  ['191125_PR/Fly1/001_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/002_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/003_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly1/004_prism/behData/images/bottom_view_prism_data_191125_PR_Fly1/',\n",
    "                  '191125_PR/Fly2/001_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/002_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/003_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/',\n",
    "                  '191125_PR/Fly2/004_prism/behData/images/bottom_view_prism_data_191125_PR_Fly2/']\n",
    "\n",
    "assert len(videos_side)==len(videos_bottom), 'Number of video files must be the same from side and bottom!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop_pos(file):\n",
    "    f=open(file, \"r\")\n",
    "    contents =f.readlines()\n",
    "    im_file = []\n",
    "    x_pos = []\n",
    "    for i in range(4,len(contents)):\n",
    "        line = contents[i][:-1].split(' ')\n",
    "        im_file.append(line[0])\n",
    "        x_pos.append(line[1])\n",
    "        \n",
    "    return im_file, x_pos\n",
    "\n",
    "\n",
    "def orientation(img):\n",
    "    #_, img_th = cv2.threshold(img, 250, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    img_th = img.copy()\n",
    "    img_th[img_th < 130] = 0 # was 140\n",
    "    # Find all the contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(img_th, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    if len(contours) < 1 : return None\n",
    "    for i, contour in enumerate(contours):\n",
    "        # Calculate the area of each contour\n",
    "        area = cv2.contourArea(contour)\n",
    "        # Ignore contours that are too small or too large\n",
    "        if area > 10000:\n",
    "            break\n",
    "\n",
    "    # Find the orientation of each shape\n",
    "    img_pts = np.empty((len(contour), 2), dtype=np.float64)\n",
    "    img_pts[:,0], img_pts[:,1] = contour[:,0,0], contour[:,0,1]\n",
    "\n",
    "    # PCA analysis\n",
    "    mean = np.empty((0))\n",
    "    _, eigenvectors, _ = cv2.PCACompute2(img_pts, mean)\n",
    "\n",
    "    angle = atan2(eigenvectors[0,1], eigenvectors[0,0])\n",
    "    return angle\n",
    "\n",
    "\n",
    "def center_and_align(pts2d, exp):\n",
    "    '''rotate align data'''\n",
    "    \n",
    "    #access corresponding image file\n",
    "    idx = pts2d.name \n",
    "    im_file, _ = read_crop_pos(home_dir + crop_positions[exp])\n",
    "    im_crop_bottom = cv2.imread(data_dir + images_bottom[exp] + im_file[idx],0)\n",
    "    \n",
    "    #get orientation and centre\n",
    "    angle = orientation(im_crop_bottom)\n",
    "    c = np.array(im_crop_bottom.shape)/2\n",
    "    \n",
    "    #rotate points\n",
    "    cos, sin = np.cos(angle), np.sin(angle)\n",
    "    R = np.array(((cos, -sin), (sin, cos)))    \n",
    "    tmp = pts2d.to_numpy().reshape(-1, 2)\n",
    "    tmp = np.matmul(tmp-c,R) + c   \n",
    "    pts2d.iloc[:] = tmp.reshape(-1,tmp.shape[0]*2).flatten()\n",
    "        \n",
    "    return pts2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for high quality frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [00:20<02:25, 20.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [01:31<03:33, 35.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [02:24<03:24, 40.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [02:38<02:11, 32.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:56<01:25, 28.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [03:00<00:42, 21.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [03:07<00:16, 16.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [03:11<00:00, 23.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "th1 = 0.95 #confidence threshold\n",
    "th2 = 10 #max L-R discrepancy in x coordinate\n",
    "\n",
    "index = []\n",
    "side = pd.DataFrame()\n",
    "bottom = pd.DataFrame()\n",
    "for i in tqdm(range(len(videos_side))):\n",
    "    _side = pd.read_hdf(home_dir + videos_side[i] + scorer_side + '.h5')\n",
    "    _bottom = pd.read_hdf(home_dir + videos_bottom[i] + scorer_bottom + '.h5')\n",
    "    \n",
    "    #drop scorer column label\n",
    "    _side = _side.droplevel('scorer',axis=1) \n",
    "    _bottom = _bottom.droplevel('scorer',axis=1) \n",
    "    \n",
    "    #split L and R (remove if we include flipping)\n",
    "    side_L_lk = _side.loc[:,(leg_tips[:3],'likelihood')]\n",
    "    side_R_lk = _side.loc[:,(leg_tips[3:],'likelihood')]\n",
    "    bottom_lk = _bottom.loc[:,(leg_tips,'likelihood')]\n",
    "    \n",
    "    #select for high likelihood frames\n",
    "    #mask = ((side_L_lk>th1).sum(1)==3) & ((bottom_lk>th1).sum(1)==6) #only flies pointing left\n",
    "    #mask = ((side_R_lk>th1).sum(1)==3) & ((bottom_lk>th1).sum(1)==6) #only flies pointing right\n",
    "    #mask = ( ((side_L_lk>th1).sum(1)==3) | ((side_R_lk>th1).sum(1)==3) ) & ((bottom_lk>th1).sum(1)==6)\n",
    "    mask = ( (((side_L_lk>th1).sum(1)==3) & ((side_R_lk>th1).sum(1)==0)) | \\\n",
    "             (((side_R_lk>th1).sum(1)==3) & ((side_L_lk>th1).sum(1)==0)) ) & \\\n",
    "             ((bottom_lk>th1).sum(1)==6)\n",
    "    _side = _side[mask].dropna()\n",
    "    _bottom = _bottom[mask].dropna()\n",
    "    \n",
    "    #sometimes DLC mixes up limbs so take only those frames there the x coordinate matches on side and bottom views\n",
    "    diff_L = np.abs(_bottom.loc[:,(leg_tips[:3],'x')].values - _side.loc[:,(leg_tips[3:],'x')].values)\n",
    "    diff_R = np.abs(_bottom.loc[:,(leg_tips[3:],'x')].values - _side.loc[:,(leg_tips[:3],'x')].values)\n",
    "    mask = ((diff_L<th2).sum(1)==3) | ((diff_R<th2).sum(1)==3)\n",
    "    _side = _side[mask].dropna()\n",
    "    _bottom = _bottom[mask].dropna()\n",
    "    \n",
    "    #take only those frames with at least 10 consecutive frames\n",
    "    if 0:\n",
    "        from itertools import groupby\n",
    "        from operator import itemgetter\n",
    "        data = list(_bottom.index)\n",
    "        epochs = []\n",
    "        for k, g in groupby(enumerate(data), lambda ix : ix[0] - ix[1]):\n",
    "            epochs.append(list(map(itemgetter(1), g)))\n",
    "        long_epochs = []\n",
    "        for e in range(len(epochs)-1):\n",
    "            if (len(epochs[e]) >= 5) | ((len(epochs[e])<5) & ((epochs[e+1][0] - epochs[e][-1]<5))):\n",
    "                long_epochs += epochs[e]\n",
    "                long_epochs += epochs[e+1]\n",
    "            \n",
    "        long_epochs=np.unique(long_epochs)  \n",
    "    \n",
    "        _bottom=_bottom.loc[long_epochs,:]\n",
    "        _side=_side.loc[long_epochs,:]\n",
    "        print('long epochs')\n",
    "    \n",
    "    #save indices for later\n",
    "    index.append(_bottom.index.values)\n",
    "    \n",
    "    assert _side.shape[0]==_bottom.shape[0], 'Number of rows must match in filtered data!'\n",
    "    \n",
    "    #flip left and right side due to prism reflection\n",
    "    cols = list(_side.columns)\n",
    "    half = int(len(cols)/2)\n",
    "    tmp = _bottom.loc[:,cols[:half]].values\n",
    "    _bottom.loc[:,cols[:half]] = _bottom.loc[:,cols[half:]].values\n",
    "    _bottom.loc[:,cols[half:]] = tmp\n",
    "    \n",
    "    #align horizontally\n",
    "    side_R_lk = _side.loc[:,(leg_tips[3:],'likelihood')] #high confidence on R joints means fly points right\n",
    "    flip_idx = (side_R_lk>th1).sum(1)==3\n",
    "    \n",
    "    if 1:\n",
    "        _bottom.loc[:,(slice(None),['x','y'])] = _bottom.loc[:,(slice(None),['x','y'])].apply(lambda x: center_and_align(x, i), axis=1)\n",
    "    \n",
    "        #rotate flies pointing right\n",
    "        theta = np.radians(180)\n",
    "        cos, sin = np.cos(theta), np.sin(theta)\n",
    "        R = np.array(((cos, -sin), (sin, cos)))     \n",
    "    \n",
    "        if np.sum(flip_idx) != 0:\n",
    "            tmp = _bottom.loc[flip_idx,(slice(None),['x','y'])].to_numpy()\n",
    "            tmp = np.reshape(tmp, [-1, 2])\n",
    "            mu = tmp.mean(axis=0)\n",
    "            tmp = np.matmul(tmp-mu,R) + mu\n",
    "            tmp = np.reshape( tmp, [-1, 60] )\n",
    "            _bottom.loc[flip_idx,(slice(None),['x','y'])] = tmp\n",
    "\n",
    "    if 0:\n",
    "        #augment with rotated flies +/- 10 deg\n",
    "        tmp = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "        _bottom_old = _bottom.copy()\n",
    "        _side_old = _side.copy()\n",
    "        flip_idx_old = flip_idx.copy()\n",
    "        for angle in [-10, 10]: \n",
    "            _bottom_rot = _bottom_old.copy()\n",
    "            theta = np.radians(angle)\n",
    "            cos, sin = np.cos(theta), np.sin(theta)\n",
    "            R1 = np.array(((cos, -sin), (sin, cos)))\n",
    "    \n",
    "            tmp1 = np.reshape(tmp, [-1, 2])\n",
    "            mu = tmp1.mean(axis=0)\n",
    "            tmp1 = np.matmul(tmp1-mu,R1) + mu\n",
    "            tmp1 = np.reshape( tmp1, [-1, 60] )\n",
    "            _bottom_rot.loc[:,(slice(None),['x','y'])] = tmp1\n",
    "            _bottom=_bottom.append(_bottom_rot) #append\n",
    "            _side=_side.append(_side_old)\n",
    "            flip_idx = flip_idx.append(flip_idx_old)\n",
    "       \n",
    "    if 0:\n",
    "        #augment with noise perturbation\n",
    "        tmp = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "        _bottom_old = _bottom.copy()\n",
    "        _side_old = _side.copy()\n",
    "        flip_idx_old = flip_idx.copy()\n",
    "        for angle in range(5): \n",
    "            _bottom_noise = _bottom_old.copy()\n",
    "            _bottom_noise.loc[:,(slice(None),['x','y'])] = tmp + np.random.normal(0,6,size=tmp.shape)\n",
    "            _bottom=_bottom.append(_bottom_noise) #append\n",
    "            _side=_side.append(_side_old)\n",
    "            flip_idx = flip_idx.append(flip_idx_old)\n",
    "    \n",
    "    #convert & save to DF3D format\n",
    "    side_np = _side.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "    z = _side.loc[:,(slice(None),'y')].to_numpy()\n",
    "    side_np = np.stack((side_np[:,::2], side_np[:,1::2]), axis=2)\n",
    "\n",
    "    bottom_np = _bottom.loc[:,(slice(None),['x','y'])].to_numpy()\n",
    "    bottom_np = np.stack((bottom_np[:,::2], bottom_np[:,1::2]), axis=2)\n",
    "    \n",
    "    poses = {'points2d': np.stack((bottom_np, side_np), axis=0),\n",
    "             'points3d': np.concatenate((bottom_np, z[:,:,None]), axis=2),\n",
    "             'index':_side.index.values,\n",
    "             'flip_idx': np.array(flip_idx)\n",
    "            }\n",
    "    \n",
    "    #save\n",
    "    pickle.dump(poses,open(home_dir + videos_side[i].split('/')[-1][6:] + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
